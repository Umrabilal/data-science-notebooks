{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745197b1-1983-4730-8fbc-ef37ad7dcfbe",
   "metadata": {
    "id": "745197b1-1983-4730-8fbc-ef37ad7dcfbe"
   },
   "source": [
    "# This is a Spam mail detection project\n",
    "# we are going to classify from the dataset contain with spam and Ham which email is Spam or ham\n",
    "# 1-Problem defination\n",
    "# 2-Dataset\n",
    "# 3-Evaluation\n",
    "# 4-Features\n",
    "# 5-Modelling\n",
    "# 6-Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f3206-3d8e-455f-bb00-0e3566b810a5",
   "metadata": {
    "id": "fa3f3206-3d8e-455f-bb00-0e3566b810a5"
   },
   "source": [
    "# 1-Problem defination\n",
    "    based on the given data we need to predict which email is Spam or not spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce5dc1-72cc-4eff-b313-e65fa562b32a",
   "metadata": {
    "id": "32ce5dc1-72cc-4eff-b313-e65fa562b32a"
   },
   "source": [
    "# 2- Dataset\n",
    "    We have the data and already have loaded it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca99cc0-506a-4be7-95b1-b03989e15565",
   "metadata": {
    "id": "1ca99cc0-506a-4be7-95b1-b03989e15565"
   },
   "source": [
    "# 3-Evaluate\n",
    "    In initial stages we need to make sure if our model gave us the accuracy of about 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc128b-e5ca-4a78-9b92-19d7742a3e31",
   "metadata": {
    "id": "8cfc128b-e5ca-4a78-9b92-19d7742a3e31"
   },
   "source": [
    "# 4-Features\n",
    "    what feature are important and what feature column means what?\n",
    "    label\n",
    "    \n",
    "Labels of Emails which can be either Spam or Ham\n",
    "    \n",
    "\n",
    "te    xt\n",
    "Emails da    ta\n",
    "\n",
    "labe    l_num\n",
    "if spam it's 1, or else it's 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d93e52-332e-4856-8daf-5a9185e5e6c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17873,
     "status": "ok",
     "timestamp": 1760826733298,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "92d93e52-332e-4856-8daf-5a9185e5e6c0",
    "outputId": "941b23d3-72fc-4778-8574-4053e14f068e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002afbba-1e65-4da8-8d8f-4253bbdfcb5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12078,
     "status": "ok",
     "timestamp": 1760826745380,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "002afbba-1e65-4da8-8d8f-4253bbdfcb5b",
    "outputId": "f4404993-8325-4895-fcdd-1207029a1c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c62b26a9-77b0-488b-afca-23376088fd78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11265,
     "status": "ok",
     "timestamp": 1760826756644,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "c62b26a9-77b0-488b-afca-23376088fd78",
    "outputId": "35c4f798-f3bd-4888-a7b7-ae7f44ff5f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from wordcloud) (2.0.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from wordcloud) (11.3.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from wordcloud) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "701277e6-db38-4a47-92f8-89fc416e9f4b",
   "metadata": {
    "executionInfo": {
     "elapsed": 4952,
     "status": "ok",
     "timestamp": 1760826761598,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "701277e6-db38-4a47-92f8-89fc416e9f4b"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np                                                    # For numerical operations\n",
    "import pandas as pd                                                   # For data manipulation and analysis\n",
    "import string                                                         # Filter out all of the puncuation from the email text\n",
    "\n",
    "\n",
    "import nltk                                                           # we need have to download something\n",
    "# Importing NLTK libraries for text preprocessing\n",
    "from nltk.corpus import stopwords                                     # For accessing a list of common stopwords\n",
    "from nltk.stem.porter import PorterStemmer                            # For stemming words to their root forms\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer           # For converting text data into numerical feature vectors\n",
    "from sklearn.model_selection import train_test_split                  # For splitting data into training and testing sets\n",
    "\n",
    "\n",
    "# For building a random forest classifier model\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a420f50-6d4a-49d5-acda-92ea80f40731",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1760826761730,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "2a420f50-6d4a-49d5-acda-92ea80f40731",
    "outputId": "231b7dca-104d-4355-fd10-cba2af03cb72"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading NLTK's stopwords corpus\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18519d7-a996-4b4f-bd18-66ad0f6abe24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 81,
     "status": "error",
     "timestamp": 1760826761813,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "e18519d7-a996-4b4f-bd18-66ad0f6abe24",
    "outputId": "3558e6a0-40c4-4972-90f2-73afeeb1e71c"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'spam_ham_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1837971679.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reading the dataset from a CSV file into a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spam_ham_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spam_ham_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Reading the dataset from a CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('spam_ham_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85e80f-006a-4b36-b479-0aaf5ff9b68e",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1760826761876,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "4a85e80f-006a-4b36-b479-0aaf5ff9b68e"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c8381-1fa3-40c0-af40-e38808dcf24a",
   "metadata": {
    "executionInfo": {
     "elapsed": 129,
     "status": "aborted",
     "timestamp": 1760826762007,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "900c8381-1fa3-40c0-af40-e38808dcf24a"
   },
   "outputs": [],
   "source": [
    "# Removing newline characters '\\r\\n' from the 'text' column and replacing them with spaces\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('\\r\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4eb95-50e4-4a02-af91-53299ffcfaa3",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1760826762018,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "8fa4eb95-50e4-4a02-af91-53299ffcfaa3"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e1ba5-80f8-4485-a521-6b1b7371dff4",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1760826762037,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "175e1ba5-80f8-4485-a521-6b1b7371dff4"
   },
   "outputs": [],
   "source": [
    "# Accessing the text from the first row of the 'text' column in the DataFrame\n",
    "df.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ab6d8-5d86-4b4f-a54a-09c8516ad69b",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "aborted",
     "timestamp": 1760826762042,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "262ab6d8-5d86-4b4f-a54a-09c8516ad69b"
   },
   "outputs": [],
   "source": [
    "# Accessing the text from the second row of the 'text' column in the DataFrame\n",
    "df.text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1e5fb-3a14-4c4e-a260-ba20997c8e14",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1760826762043,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "30b1e5fb-3a14-4c4e-a260-ba20997c8e14"
   },
   "outputs": [],
   "source": [
    "# Accessing the text from the third row of the 'text' column in the DataFrame\n",
    "df.text.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa18da-37ac-4bd9-b3fc-2d43d6f4eb9d",
   "metadata": {
    "executionInfo": {
     "elapsed": 46832,
     "status": "aborted",
     "timestamp": 1760826762044,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "fefa18da-37ac-4bd9-b3fc-2d43d6f4eb9d"
   },
   "outputs": [],
   "source": [
    "# Displaying summary information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1442b8-1075-40ac-b8db-7b0261966508",
   "metadata": {
    "executionInfo": {
     "elapsed": 46834,
     "status": "aborted",
     "timestamp": 1760826762046,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "fb1442b8-1075-40ac-b8db-7b0261966508"
   },
   "outputs": [],
   "source": [
    "# Initializing a Porter stemmer and creating a corpus of stemmed text data\n",
    "stemmer = PorterStemmer()                                                        # Initialize a Porter stemmer for word stemming\n",
    "corpus = []                                                                      # Initialize an empty list to store the processed text data\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))                                  # Get a set of English stopwords\n",
    "\n",
    "# Iterate through each row of the DataFrame to preprocess the text data\n",
    "for i in range(len(df)):\n",
    "     # Convert the text to lowercase and tokenize it\n",
    "    text = df['text'].iloc[i].lower()\n",
    "    text = text.translate(str.maketrans(' ', ' ', string.punctuation)).split()\n",
    "    # Apply stemming to each word in the text and remove stopwords\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords_set]\n",
    "    # Join the stemmed words back into a single string and add it to the corpus\n",
    "    text = ' '.join(text)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8dc6d6-d57c-443b-ab36-ef4b2f747224",
   "metadata": {
    "executionInfo": {
     "elapsed": 46831,
     "status": "aborted",
     "timestamp": 1760826762047,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "9d8dc6d6-d57c-443b-ab36-ef4b2f747224"
   },
   "outputs": [],
   "source": [
    "# Accessing the preprocessed text data of the second document in the corpus\n",
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0372e91-a2c0-44d8-b4b6-528bda05a3a7",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1760826762049,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "c0372e91-a2c0-44d8-b4b6-528bda05a3a7"
   },
   "outputs": [],
   "source": [
    "df.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a5f69-3836-4d0b-ba64-e528b3dc1206",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1760826762051,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "c00a5f69-3836-4d0b-ba64-e528b3dc1206"
   },
   "outputs": [],
   "source": [
    "# Creating feature vectors using CountVectorizer and splitting the data into training and testing sets\n",
    "vectorizer = CountVectorizer()                                             # Initialize CountVectorizer for converting text to feature vectors\n",
    "x = vectorizer.fit_transform(corpus).toarray()                             # Convert text corpus to a matrix of token counts\n",
    "y = df.label_num                                                           # Assigning labels from the DataFrame to y\n",
    "\n",
    "# Splitting the data into training and testing sets with a ratio of 80% training and 20% testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size =0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637b6f9-67e2-4637-9c2d-0d420a0d7d09",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1760826762052,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "d637b6f9-67e2-4637-9c2d-0d420a0d7d09"
   },
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d0fa8-a2ea-46d2-918a-0ee8452f1d15",
   "metadata": {
    "executionInfo": {
     "elapsed": 46823,
     "status": "aborted",
     "timestamp": 1760826762053,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "c79d0fa8-a2ea-46d2-918a-0ee8452f1d15"
   },
   "outputs": [],
   "source": [
    "# Printing the shapes of the training data and corresponding labels\n",
    "print(\"Shape of x_train:\", x_train.shape)                              # Print the shape of the training data\n",
    "print(\"Shape of y_train:\", y_train.shape)                              # Print the shape of the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac7ac1-1416-4c31-95f5-abbb6f213ef9",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1760826762069,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "70ac7ac1-1416-4c31-95f5-abbb6f213ef9"
   },
   "outputs": [],
   "source": [
    "# Initializing a RandomForestClassifier and fitting it to the training data\n",
    "clf = RandomForestClassifier(n_jobs=-1)                               # Initialize a RandomForestClassifier with parallel processing\n",
    "clf.fit(x_train, y_train)                                             # Fit the classifier to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dfba69-1243-4b28-a2a5-ee35de5fc424",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1760826762072,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "f6dfba69-1243-4b28-a2a5-ee35de5fc424"
   },
   "outputs": [],
   "source": [
    "# Calculating the accuracy score of the trained classifier on the testing data\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe65935-0ad9-4ad0-8f3d-f7aec0d6897e",
   "metadata": {
    "executionInfo": {
     "elapsed": 46833,
     "status": "aborted",
     "timestamp": 1760826762074,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "ffe65935-0ad9-4ad0-8f3d-f7aec0d6897e"
   },
   "outputs": [],
   "source": [
    "# Assigning the text of the 11th document in the DataFrame to the variable 'email_to_classify'\n",
    "email_to_classify = df.text.values[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c231eacc-f229-4770-bb4a-931083cb0e10",
   "metadata": {
    "executionInfo": {
     "elapsed": 46830,
     "status": "aborted",
     "timestamp": 1760826762075,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "c231eacc-f229-4770-bb4a-931083cb0e10"
   },
   "outputs": [],
   "source": [
    "email_to_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f291a77-7545-4a93-bd22-3909e1195bf4",
   "metadata": {
    "executionInfo": {
     "elapsed": 46830,
     "status": "aborted",
     "timestamp": 1760826762076,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "1f291a77-7545-4a93-bd22-3909e1195bf4"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the text of the email to classify\n",
    "# 1. Convert the email text to lowercase, remove punctuation, and split into words\n",
    "email_text = email_to_classify.lower().translate(str.maketrans(\" \",\" \", string.punctuation)).split()\n",
    "# 2. Apply stemming to each word and remove stopwords\n",
    "email_text = [stemmer.stem(word) for word in email_text if word not in stopwords_set]\n",
    "# 3. Join the stemmed words back into a single string\n",
    "email_text = ' '.join(email_text)\n",
    "# 4. Create a corpus containing the preprocessed email text\n",
    "email_corpus = [email_text]\n",
    "# 5. Transform the email text into a feature vector using the same vectorizer as used for training data\n",
    "x_email = vectorizer.transform(email_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f730d9-adc0-4eae-ad81-e1d7ddcfcd51",
   "metadata": {
    "executionInfo": {
     "elapsed": 46828,
     "status": "aborted",
     "timestamp": 1760826762077,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "54f730d9-adc0-4eae-ad81-e1d7ddcfcd51"
   },
   "outputs": [],
   "source": [
    "# Predicting the label of the email text using the trained classifier\n",
    "clf.predict(x_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cf8c9-2e01-414a-a38e-bdd537e5739a",
   "metadata": {
    "executionInfo": {
     "elapsed": 46829,
     "status": "aborted",
     "timestamp": 1760826762082,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "153cf8c9-2e01-414a-a38e-bdd537e5739a"
   },
   "outputs": [],
   "source": [
    "# Accessing the label of the 16th document in the DataFrame\n",
    "df.label_num.iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73197e9-5627-45c1-b8aa-c7a552420eb1",
   "metadata": {
    "executionInfo": {
     "elapsed": 46830,
     "status": "aborted",
     "timestamp": 1760826762084,
     "user": {
      "displayName": "Bilal Ahmad",
      "userId": "18275899233461807804"
     },
     "user_tz": 420
    },
    "id": "b73197e9-5627-45c1-b8aa-c7a552420eb1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
